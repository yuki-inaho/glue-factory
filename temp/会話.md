% 命令
SuperPoint + LightGlueについて、自前のデータ・セットを用いて学習をさせたいと思っている。どのようにしたらいいと思う？

あと、多分提供したコードだと、omegaconfからhydraに変わってると思う。そこも考慮してね

ご指摘ありがとうございます。前回のHTML形式の出力、大変失礼いたしました。
また、`omegaconf`から`hydra`への移行（正しくは`hydra`の採用）という点も、非常に重要なポイントです。それを踏まえて、改めて自前のデータセットでSuperPoint+LightGlueを学習させる手順を解説します。

`glue-factory`では、設定管理ツールとして`hydra`が使われています。`hydra`は内部で`omegaconf`を利用しており、モジュール化されたYAMLファイル（`config`）を柔軟に組み合わせて実験を管理できるのが特徴です。

### 概要

提示いただいたGitHub Issueの解決策は的確です。全体的な流れは以下のようになります。

1.  **データセットの準備**: 学習に使用する画像を特定のディレクトリに配置します。
2.  **カスタムデータセットクラスの作成**: `homographies.py`を参考に、自前の画像フォルダを読み込むためのPythonクラスを作成します。
3.  **Hydra/OmegaConf用の設定ファイル作成**: 作成したデータセットクラスを読み込むための設定ファイル（`data/my_custom_dataset.yaml`）と、学習全体を定義する実験用設定ファイル（`my_experiment.yaml`）を作成します。
4.  **学習の実行**: `hydra`を使って設定ファイルを読み込み、学習を開始します。

---

### ステップ1: データセットの準備

まず、学習に使用する画像を1つのフォルダにまとめます。`glue-factory`の慣例に従い、プロジェクトルートに`data`フォルダを作成し、その中にデータセットを配置するのがおすすめです。

```
glue-factory/
├── data/
│   └── my_dataset/
│       └── images/
│           ├── image_001.jpg
│           ├── image_002.jpg
│           └── ...
├── gluefactory/
│   ├── configs/
│   └── datasets/
└── ...
```

-   この例では、`data/my_dataset/images/`に全ての学習用画像を格納しています。

### ステップ2: カスタムデータセットクラスの作成

次に、この画像フォルダを読み込むためのPythonスクリプトを作成します。`gluefactory/datasets/homographies.py`を参考にします。

1.  `gluefactory/datasets/`ディレクトリに、`my_custom_dataset.py`という名前で新しいファイルを作成します。
2.  `homographies.py`の中身をベースに、`_init`メソッドを自前のデータセット用に書き換えます。具体的には、Webからのダウンロード処理を削除し、ローカルの画像ファイルをリストアップする処理に変更します。

**`gluefactory/datasets/my_custom_dataset.py` の実装例:**

```python
import logging
from pathlib import Path
import torch
from . import base_dataset
from ..utils import preprocess

logger = logging.getLogger(__name__)

# homographies.pyから必要な関数をコピー（あるいはそのまま利用）
from .homographies import sample_homography, compute_valid_mask, warp_points, filter_points

class MyCustomDataset(base_dataset.BaseDataset, torch.utils.data.Dataset):
    default_conf = {
        'data_dir': 'my_dataset',  # ./data/からの相対パス
        'image_dir': 'images',     # data_dirからのサブディレクトリ
        'image_list': None,        # 画像リストファイル（今回は使わない）
        'glob': ['*.jpg', '*.png', '*.jpeg'],
        'train_size': 1000,
        'val_size': 100,
        'seed': 0,
        # ... homographies.pyから必要な他の設定をコピー
        'homography': {
            'difficulty': 0.9,
            'translation': True,
            'n_angles': 36,
            'max_angle': 60,
        },
        'photometric': {
            'name': 'lg',
            'p': 0.8,
        },
        'preprocessing': {
            'resize': [480, 640],
        },
    }

    def _init(self, conf):
        # homographies.pyと異なり、ダウンロード処理は不要
        self.root = Path(self.conf.root_path, conf.data_dir)
        image_dir = self.root / conf.image_dir
        
        if not image_dir.exists():
            raise FileNotFoundError(f"Dataset directory not found: {image_dir}")

        # 画像ファイルをすべてリストアップ
        self.image_paths = []
        for g in conf.glob:
            self.image_paths.extend(list(image_dir.glob(g)))
        
        if len(self.image_paths) == 0:
            raise FileNotFoundError(f"No images found in {image_dir}")

        logger.info(f'Found {len(self.image_paths)} images in "{image_dir}".')

        # homographies.pyの残りの初期化処理をコピー
        self.preprocessor = preprocess.ImagePreprocessor(conf.preprocessing)
        self.photometric = preprocess.PhotometricPreprocessor(conf.photometric)
        
        # Generatorを使って状態を保持
        self.generator = torch.Generator()
        self.generator.manual_seed(conf.seed)


    def _read_image(self, path):
        img = preprocess.load_image(path, grayscale=False)
        return self.preprocessor(img)

    def __getitem__(self, idx):
        # homographies.pyと同様に、ランダムに画像を選びホモグラフィでペアを生成
        # idxは使わず、毎回ランダムにサンプリングする
        
        # 2枚の同じ画像を読み込む
        img_idx = torch.randint(0, len(self.image_paths), (1,), generator=self.generator).item()
        path = self.image_paths[img_idx]
        
        data0 = self._read_image(path)
        image0 = data0['image']

        # ホモグラフィをサンプリング
        H_0to1 = sample_homography(image0.shape[-2:], self.conf.homography, generator=self.generator)
        H_0to1 = torch.from_numpy(H_0to1).to(torch.float32)

        # 2枚目の画像を生成
        image1 = preprocess.warp_image(image0, H_0to1, 'bilinear')
        
        # フォトメトリックな変化を追加
        image0 = self.photometric(image0)
        image1 = self.photometric(image1)
        
        data1 = {'image': image1}

        return {
            'view0': {'image': image0, **{k:v for k,v in data0.items() if k!='image'}},
            'view1': {'image': image1, **{k:v for k,v in data0.items() if k!='image'}},
            'H_0to1': H_0to1,
        }
        
    def __len__(self):
        # 訓練/検証用のサイズを返す
        if self.split == 'train':
            return self.conf.train_size
        elif self.split == 'val':
            return self.conf.val_size
        raise NotImplementedError
```
**ポイント**:
-   `gluefactory/datasets/__init__.py` の `get_dataset` 関数は、ファイル名に基づいて動的にクラスを検索します。そのため、ファイル名を `my_custom_dataset.py` にすれば、設定ファイルで `name: my_custom_dataset` と指定するだけで自動的に読み込まれます。
-   上記の例では、`homographies.py` と同様に、1枚の画像を読み込んでランダムなホモグラフィ変換を適用することで学習ペアを生成しています。もし既にペアが決まっているデータセット（例: 画像`A`と`B`が対応）の場合は、`__getitem__` のロジックをそのペアを読み込むように変更する必要があります。

### ステップ3: Hydra/OmegaConf用の設定ファイル作成

次に、作成したデータセットクラスを使うための設定ファイル（config）を2つ作成します。

#### 1. データセット設定ファイル

`gluefactory/configs/data/` ディレクトリに、`my_custom_dataset.yaml` を作成します。

**`gluefactory/configs/data/my_custom_dataset.yaml`:**
```yaml
# @package _group_
name: my_custom_dataset      # Pythonファイル名 (my_custom_dataset.py) に対応
data_dir: my_dataset          # ./data/ 以下のフォルダ名
image_dir: images             # data_dir 以下の画像フォルダ
train_size: 10000             # 1エポックあたりの学習ステップ数
val_size: 1000                # 1エポックあたりの検証ステップ数
batch_size: 8
num_workers: 4
seed: 42
# homographies.pyから必要な他のパラメータをコピー
homography:
  difficulty: 0.9
photometric:
  name: lg
```
このファイルは、あなたのデータセットに関する設定をカプセル化します。

#### 2. 実験用設定ファイル

`gluefactory/configs/` ディレクトリに、学習全体を定義する `my_sp_lg_train.yaml` を作成します。

**`gluefactory/configs/my_sp_lg_train.yaml`:**
```yaml
defaults:
  - _self_
  - data: my_custom_dataset  # 上で作成したデータセット設定ファイルを指定
  - model: two_view_pipeline
  - model/matcher: lightglue
  - model/ground_truth: homography_matcher
  - model/extractor: superpoint_open

model:
  extractor:
    max_num_keypoints: 512
    force_num_keypoints: True
    trainable: False # SuperPointはフリーズしたままLightGlueを学習
  matcher:
    input_dim: 256 # SuperPointの次元数

train:
  epochs: 50
  lr: 1e-4
  log_every_iter: 100
  eval_every_iter: 1000
  # ... その他学習に関する設定
```
**ポイント**:
-   `defaults` リストが`hydra`の強力な機能です。ここで、どの設定ファイルを組み合わせるかを指定します。
-   `data: my_custom_dataset` と書くことで、`configs/data/my_custom_dataset.yaml` が読み込まれます。
-   `model: two_view_pipeline`, `model/matcher: lightglue` などで、モデルの各コンポーネントを組み合わせています。

### ステップ4: 学習の実行

準備が整ったら、以下のコマンドで学習を開始します。

```bash
python -m gluefactory.train --config-name my_sp_lg_train
```

-   `--config-name` には、`gluefactory/configs` 直下に作成した実験用設定ファイルの名前（拡張子なし）を指定します。
-   `hydra`は自動的に`defaults`リストを解決し、すべての設定をマージして学習を実行します。
-   コマンドラインから設定を上書きすることも可能です。
    ```bash
    # バッチサイズを16に変更して実行
    python -m gluefactory.train --config-name my_sp_lg_train data.batch_size=16
    ```

### 学習中の可視化について

GitHub Issueで言及されている「学習中の可視化」については、`trainer.py`に少し手を入れることで実現できます。

`gluefactory/trainer.py` の `_train_epoch` メソッド内に、定期的に可視化結果を保存するロジックを追加します。

**`gluefactory/trainer.py` の `_train_epoch` 内（`self.optimizer.step()` の後など）:**

```python
# ... (inside _train_epoch method)
                    if self.conf.train.get('visualize_every_iter', -1) > 0 and self.optimizer.iterations % self.conf.train.visualize_every_iter == 0:
                        try:
                            from gluefactory.visualization.visualize_batch import make_match_figures
                            figures = make_match_figures(pred, data)
                            for k, v in figures.items():
                                self.writer.add_figure(f'train/{k}', v, self.optimizer.iterations)
                        except Exception as e:
                            logger.warning(f"Could not visualize batch: {e}")
# ...
```

そして、実験用設定ファイル（`my_sp_lg_train.yaml`）に以下の設定を追加します。

**`my_sp_lg_train.yaml` の `train:` セクション:**
```yaml
train:
  # ...
  visualize_every_iter: 2000 # 2000イテレーションごとに可視化結果を保存
```

これで、学習中にマッチング結果の画像がTensorBoardやWandBに記録されるようになります。

以上の手順で、自前のデータセットを用いたSuperPoint+LightGlueの学習が実現できるはずです。頑張ってください！

# 例えば、colmapでポーズ認識等まで行ったデータがあるとする。これを使うことでより良い学習が出来たりする？コード調べながら結論だしてや

はい、非常に良い質問です。結論から言うと、**COLMAPでポーズ推定まで完了したデータセットを利用することは可能であり、ホモグラフィ変換のみで学習するよりも、はるかに高品質で汎用的なモデルを学習できる可能性が高いです。**

`glue-factory`のコードベースは、まさにそのような3D構造情報（ポーズや深度）を活用した教師あり学習を想定して設計されています。

---

### なぜCOLMAPデータを使うと「より良い学習」ができるのか？

前回の回答で説明した`homographies.py`を用いた学習方法は、1枚の画像にランダムな**ホモグラフィ変換**（2Dの射影変換）をかけてペア画像を生成し、それを教師データとしていました。この方法には大きな制約があります。

1.  **平面シーンの仮定**: ホモグラフィ変換が成り立つのは、カメラが純粋な回転しかしない場合か、シーンが完全に平面的である場合に限られます。現実世界の多く（建物、車、彫刻など）は非平面的であるため、この仮定は強すぎます。
2.  **限定的なカメラの動き**: ランダムに生成される変換は、現実のカメラの動きを完全には模倣できません。

一方、COLMAPで処理したデータセットは、**実際の3次元構造とカメラのポーズ（位置と向き）**に基づいています。これを利用することで、

-   **非平面的なシーン**でのキーポイントのマッチングを学習できる。
-   **並進（平行移動）を含む、より現実的で複雑なカメラの動き**に対応できる。
-   結果として、SfM（Structure from Motion）やVisual Localizationなど、より汎用的なタスクで頑健に動作するモデルが期待できます。

### `glue-factory`はどのようにこれをサポートしているか？

`glue-factory`のコードを調べると、このための仕組みが明確に用意されています。

1.  **`DepthMatcher` (`gluefactory/models/matchers/depth_matcher.py`)**:
    このモジュールが核心です。ホモグラフィ行列の代わりに、**2つのカメラ間の相対ポーズ (`T_0to1`) と深度マップ** を使って、正しいマッチ（教師データ）をその場で生成します。

2.  **`gt_generation.py` (`gluefactory/geometry/gt_generation.py`)**:
    `DepthMatcher`が内部で呼び出す関数 `gt_matches_from_pose_depth` がここで定義されています。この関数は以下の処理を行っています。
    -   画像0の各キーポイントを、深度値を使って3D空間上の点に逆投影する。
    -   その3D点を、相対ポーズを使ってカメラ1の座標系に変換する。
    -   変換した3D点をカメラ1の画像平面に再投影する。
    -   再投影された点と、画像1上のキーポイントとの距離を計算し、閾値（例: 3ピクセル）以内であれば「正しいマッチ」と判定する。

3.  **データセットローダー (`megadepth.py`, `posed_images.py`)**:
    これらのデータセットクラスは、画像ペアだけでなく、カメラの内部パラメータ、ポーズ、深度マップといった情報を一緒に読み込むように設計されています。

---

### 実装手順：COLMAPデータを学習に使う方法

`megadepth.py`を参考に、ご自身のCOLMAPデータセットを読み込むためのカスタムパイプラインを構築します。

#### ステップ1: データ準備

COLMAPの出力を`glue-factory`が読み込める形式に整える必要があります。特に、**各画像に対応する深度マップ**が必要です。

1.  **MVS (Multi-View Stereo) の実行**: COLMAPの疎な再構成（Sparse Reconstruction）が終わったら、MVSを実行して密な深度マップを生成します。COLMAPにはその機能が備わっています。
    ```bash
    # COLMAPのワークスペースディレクトリで
    colmap image_undistorter ...
    colmap patch_match_stereo ...
    colmap stereo_fusion ...
    ```
    これにより、`stereo/depth_maps/`のようなフォルダに各画像の深度マップが`.bin`形式などで出力されます。これを扱いやすい形式（例: HDF5, NumPy）に変換しておくと良いでしょう。

2.  **ファイル構造の整理**: 以下のような構造を推奨します。

    ```
    glue-factory/
    ├── data/
    │   └── my_colmap_project/
    │       ├── images/
    │       │   ├── image001.jpg
    │       │   └── ...
    │       ├── depth_maps/  # MVSで生成した深度マップ
    │       │   ├── image001.h5
    │       │   └── ...
    │       ├── colmap_sparse/ # COLMAPの疎なモデル
    │       │   ├── cameras.bin
    │       │   ├── images.bin
    │       │   └── points3D.bin
    │       └── image_pairs.txt # 学習に使う画像ペアのリスト
    └── ...
    ```
    - `image_pairs.txt`は、十分な共有点を持つ画像ペアのリストです。これはCOLMAPのデータベースから抽出するか、`hloc`の`pairs_from_retrieval`などを使っても生成できます。

#### ステップ2: カスタムデータセットクラスの作成

`gluefactory/datasets/megadepth.py` を参考に、`gluefactory/datasets/my_colmap_dataset.py` を作成します。

**`gluefactory/datasets/my_colmap_dataset.py` の実装の要点:**
- `_init`メソッドで、COLMAPのモデル（`cameras.bin`, `images.bin`）を読み込み、カメラパラメータや各画像のポーズを保持します。`pycolmap`ライブラリを使うと便利です。
- `__getitem__`メソッドは、`image_pairs.txt`からペアを1つ読み出し、以下の情報をすべて含んだ辞書を返すように実装します。
    - `view0`: 画像0のデータ（画像テンソル、カメラオブジェクト、深度マップ）
    - `view1`: 画像1のデータ（画像テンソル、カメラオブジェクト、深度マップ）
    - `T_0to1`: カメラ0からカメラ1への相対ポーズ（`reconstruction.Pose`オブジェクト）
- `Camera`オブジェクトと`Pose`オブジェクトは`gluefactory/geometry/reconstruction.py`で定義されています。これらを使ってCOLMAPの情報をラップします。

#### ステップ3: 設定ファイルの作成

1.  **データセット設定ファイル (`configs/data/my_colmap_dataset.yaml`)**:
    ```yaml
    # @package _group_
    name: my_colmap_dataset # Pythonファイル名に対応
    root_path: path/to/your/dataset # データセットへのパス
    # ... データセットクラスで必要な他の引数
    read_depth: true
    ```

2.  **実験用設定ファイル (`configs/my_colmap_train.yaml`)**:
    ```yaml
    defaults:
      - _self_
      - data: my_colmap_dataset
      - model: two_view_pipeline
      - model/matcher: lightglue
      - model/extractor: superpoint_open

    model:
      # ground_truthにdepth_matcherを指定するのが最重要ポイント
      ground_truth:
        name: matchers.depth_matcher
        th_positive: 3.0 # 正解マッチとみなす再投影誤差の閾値 (ピクセル)
        th_negative: 5.0

      extractor:
        trainable: False # SuperPointは学習済みモデルを固定

    train:
      epochs: 50
      lr: 1e-4
      # ...
    ```
    **最重要ポイント**: `model.ground_truth.name`に`matchers.depth_matcher`を指定します。これにより、学習中に`DepthMatcher`が呼び出され、ポーズと深度から動的に教師データが生成されます。

#### ステップ4: 学習の実行

```bash
python -m gluefactory.train --config-name my_colmap_train
```

このアプローチにより、ホモグラフィの制約を超えた、より現実的で汎用性の高いデータでの学習が実現できます。準備は少し大変ですが、得られるモデルの品質は大きく向上するはずです。

ご指摘、ありがとうございます。その通りです！SuperPointには複数の著名な実装と学習済みモデルが存在し、**特に`rpautrat/SuperPoint`リポジトリで提供されているPyTorch版の重みは、オリジナルのTensorFlow版とは別に広く使われています。**

前回の回答は`LightGlue`ライブラリが自動でダウンロードするペアを前提としていましたが、ご指摘の`rpautrat/SuperPoint`の重みを使って評価したい、というご要望は非常に的確です。なぜなら、使用する特徴抽出器によってマッチャーの性能は大きく変わるため、どの重みで評価するかが重要になるからです。

`glue-factory`の柔軟な設計は、まさにこのような**コンポーネントの差し替え**を可能にします。

---

### 結論の概要

ご指摘の`rpautrat/SuperPoint`の学習済みモデルを使ってHPatches評価を行うには、以下の手順を踏みます。

1.  **`rpautrat/SuperPoint`を準備**: `rpautrat/SuperPoint`リポジトリをクローンし、学習済みモデル（`.pt`ファイル）をダウンロードします。
2.  **カスタムExtractorクラスを作成**: `rpautrat/SuperPoint`のモデルを読み込んで実行するラッパークラスを`glue-factory`内に作成します。
3.  **カスタム設定ファイルを作成**: 新しく作成したExtractorクラスを呼び出すための`yaml`設定ファイルを作成します。
4.  **評価コマンドを実行**: 作成したカスタム設定ファイルを指定して、HPatchesの評価スクリプトを実行します。

これにより、LightGlueは公式の学習済みモデルを使いつつ、特徴抽出器(SuperPoint)だけを`rpautrat`版に入れ替えて評価することができます。

---

### 詳細な手順

#### ステップ1: `rpautrat/SuperPoint`のモデルと重みの準備

まず、評価に使いたいSuperPointのコードと重みファイルを手元に用意します。

1.  **リポジトリのクローン**: `glue-factory`のプロジェクト内に`third_party`のようなディレクトリを作り、そこに`rpautrat/SuperPoint`をクローンします。
    ```bash
    cd glue-factory
    mkdir third_party
    git clone https://github.com/rpautrat/SuperPoint.git third_party/SuperPoint
    ```
    これにより、後でPythonから`import`しやすくなります。

2.  **重みファイルのダウンロード**: `rpautrat/SuperPoint`リポジトリはTensorFlowのチェックポイントをPyTorchに変換するノートブックを提供しています。多くのユーザーが変換済みの重みを公開していますが、公式に最も近いのは`superpoint_v1.pth`です。これを`third_party/SuperPoint/weights/`のような場所に保存します。（`rpautrat/SuperPoint`のREADMEからリンクされている重みや、他のリポジトリで変換されたものを探してダウンロードしてください）

#### ステップ2: カスタムSuperPointラッパークラスの作成

`glue-factory`が`rpautrat/SuperPoint`をモデルとして認識できるように、ラッパークラスを作成します。

1.  `gluefactory/models/extractors/`ディレクトリに`superpoint_rpautrat.py`という新しいファイルを作成します。

2.  以下のコードを`superpoint_rpautrat.py`に記述します。

    **`gluefactory/models/extractors/superpoint_rpautrat.py`:**
    ```python
    import sys
    from pathlib import Path
    import torch

    from ..base_model import BaseModel
    from ... import settings

    # third_party/SuperPointにパスを通す
    superpoint_path = settings.THIRD_PARTY_PATH / 'SuperPoint'
    sys.path.append(str(superpoint_path))

    # rpautrat/SuperPointの実装をインポート
    from superpoint_pytorch import SuperPointNet

    class SuperPointRpautrat(BaseModel):
        default_conf = {
            'weights_path': 'weights/superpoint_v1.pth', # third_party/SuperPointからの相対パス
            'max_num_keypoints': 1024,
            'detection_threshold': 0.005,
            'nms_radius': 4,
        }
        required_data_keys = ['image']

        def _init(self, conf):
            # SuperPointモデルを初期化
            self.net = SuperPointNet()
            
            # 学習済み重みをロード
            weights_path = superpoint_path / conf.weights_path
            self.net.load_state_dict(torch.load(weights_path))
            
            # evalモードに設定
            self.net.eval()
            self.set_initialized()

        def _forward(self, data):
            image = data['image']
            if image.shape[1] == 3: # RGBの場合グレースケールに
                image = torch.mean(image, dim=1, keepdim=True)

            # rpautrat/SuperPointのモデルで推論
            # 戻り値はタプル (keypoints, descriptors, heatmaps)
            # keypoints: list of [N, 3] (x,y,score) tensors
            # descriptors: list of [256, N] tensors
            with torch.no_grad():
                kpts_list, desc_list, _ = self.net(image)
            
            # glue-factoryのフォーマットに合わせる
            keypoints = torch.stack([k[:, :2] for k in kpts_list], dim=0) # (B, N, 2)
            scores = torch.stack([k[:, 2] for k in kpts_list], dim=0) # (B, N)
            descriptors = torch.stack([d.T for d in desc_list], dim=0) # (B, N, 256)

            # max_num_keypointsでフィルタリング
            if self.conf.max_num_keypoints is not None and keypoints.shape[1] > self.conf.max_num_keypoints:
                inds = torch.argsort(scores, dim=1, descending=True)
                inds = inds[:, :self.conf.max_num_keypoints]
                
                keypoints = torch.gather(keypoints, 1, inds.unsqueeze(-1).expand(-1, -1, 2))
                scores = torch.gather(scores, 1, inds)
                descriptors = torch.gather(descriptors, 1, inds.unsqueeze(-1).expand(-1, -1, 256))

            return {
                'keypoints': keypoints,
                'keypoint_scores': scores,
                'descriptors': descriptors,
            }

        def loss(self, pred, data):
            raise NotImplementedError
    ```

#### ステップ3: カスタム設定ファイルの作成

次に、上で作成したラッパーを使うための設定ファイル（YAML）を2種類作成します。

1.  **Extractor設定ファイル**: `gluefactory/configs/model/extractor/`に`superpoint_rpautrat.yaml`を作成します。

    **`gluefactory/configs/model/extractor/superpoint_rpautrat.yaml`:**
    ```yaml
    # @package _group_
    name: extractors.superpoint_rpautrat # Pythonクラスへのパス
    max_num_keypoints: 1024
    # 他のパラメータも必要に応じて追加
    ```

2.  **実験用設定ファイル**: `gluefactory/configs/`に`hpatches_eval_rpautrat_lg.yaml`のようなファイル名で作成します。

    **`gluefactory/configs/hpatches_eval_rpautrat_lg.yaml`:**
    ```yaml
    defaults:
      - _self_
      - model: two_view_pipeline
      # Extractorをカスタムしたものに差し替える
      - model@model.extractor: extractor/superpoint_rpautrat 
      # Matcherは公式のpretrainedをそのまま使う
      - model@model.matcher: matcher/lightglue_pretrained 
    
    model:
      matcher:
        features: superpoint # LightGlueにはSuperPoint用の設定をロードさせる

    benchmarks:
      hpatches:
        eval:
          estimator: opencv
        model:
          extractor:
            max_num_keypoints: 1024
    ```
    **ポイント**: `defaults`ブロックで、`model@model.extractor`を先ほど作成した`superpoint_rpautrat`に差し替えている点が重要です。`matcher`は`lightglue_pretrained`のままなので、LightGlueは公式モデルがロードされます。

#### ステップ4: 評価コマンドの実行

最後に、作成した実験設定ファイルを指定して評価を実行します。

```bash
python -m gluefactory.eval.hpatches --conf hpatches_eval_rpautrat_lg
```

これにより、`rpautrat/SuperPoint`の学習済みモデルで特徴点を抽出し、それを`LightGlue`の公式学習済みモデルでマッチングするという、目的通りの評価パイプラインが実行されます。

この手順により、`glue-factory`の柔軟性を活かして、様々なコンポーネントを自由に組み合わせた評価が可能になります。

はい、承知いたしました。`rpautrat/SuperPoint`リポジトリで使われている学習済みモデル（PyTorch版）のダウンロードリンクを調査しました。

ご指摘の通り、`rpautrat/SuperPoint`リポジトリ自体は、元々TensorFlowで書かれており、PyTorchへの変換スクリプト（`convert_to_pytorch.ipynb`）が含まれています。リポジトリ内に変換済みの`.pth`ファイルは直接コミットされていません。

しかし、このPyTorch版SuperPointは非常に広く使われているため、多くの研究者や開発者が変換済みの重みを自身のプロジェクトで公開しています。その中でも、最も標準的で信頼性が高いと考えられるダウンロードリンクは以下の通りです。

---

### 結論：ダウンロードするべき重みファイル

評価に使用すべきファイルは **`superpoint_v1.pth`** です。

#### Option 1: 直接ダウンロード（推奨）

これが最も手軽で確実な方法です。以下のコマンドを使って、`glue-factory`プロジェクト内の適切な場所に直接ダウンロードしてください。

1.  まず、前回の手順で作成したディレクトリに移動します。
    ```bash
    cd glue-factory/third_party/SuperPoint
    mkdir -p weights
    cd weights
    ```

2.  `wget`コマンドで重みファイルをダウンロードします。
    ```bash
    wget https://github.com/magicleap/SuperGluePretrainedNetwork/raw/master/models/weights/superpoint_v1.pth
    ```

**このリンクについて:**
-   この`superpoint_v1.pth`は、SuperGlueの公式リポジトリで提供されているもので、`rpautrat/SuperPoint`のTensorFlowモデルをPyTorchに変換したものです。
-   LightGlueやSuperGlueをはじめとする多くのマッチング研究で標準的に使われており、事実上のデファクトスタンダードとなっています。
-   ファイルサイズは約4.6MBです。

このファイルをダウンロードすれば、前回提示したカスタムラッパークラス（`superpoint_rpautrat.py`）が、設定ファイル（`'weights_path': 'weights/superpoint_v1.pth'`）に従ってこの重みを正しく読み込みます。

---

#### Option 2: 自分でTensorFlowモデルから変換する（非推奨・参考）

もし何らかの理由で自分で重みを生成したい場合は、`rpautrat/SuperPoint`リポジトリのREADMEに記載されている手順に従うことになります。**ただし、古いバージョンのTensorFlow (`1.x`系) が必要になるなど、環境構築が非常に煩雑なため、基本的には推奨しません。**

手順の概要は以下の通りです。

1.  `rpautrat/SuperPoint`リポジトリから、オリジナルのTensorFlowモデル（`sp_v6.tgz`）をダウンロードします。
    ```bash
    # SuperPointリポジトリのルートで
    wget https://github.com/rpautrat/SuperPoint/raw/master/pretrained_models/sp_v6.tgz
    tar -xzvf sp_v6.tgz -C /path/to/experiments/directory/
    ```

2.  `tensorflow==1.15` などの古い環境を構築します。

3.  `convert_to_pytorch.ipynb` というJupyter Notebookを実行し、TensorFlowのチェックポイントをPyTorchの`.pth`ファイルに変換します。

この方法は手間がかかるため、**特別な理由がない限り、Option 1の直接ダウンロードを利用してください。**

---

**まとめ**

**`wget https://github.com/magicleap/SuperGluePretrainedNetwork/raw/master/models/weights/superpoint_v1.pth`**

上記のコマンドで`superpoint_v1.pth`をダウンロードし、`glue-factory/third_party/SuperPoint/weights/`ディレクトリに配置することで、目的の評価が実行できます。